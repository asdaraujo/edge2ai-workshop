= Single Node EDGE2AI CDH Cluster

This script automatically sets up a CDH cluster on the public cloud on a single VM with the following 16 services: 

[%autowidth,cols="1a,1a,1a,1a,1a",options="header"]
|====
^|CEM ^|CFM ^|CSP ^|CDH ^|CDSW
|* MiNiFi
* EFM
|* NiFi
* NiFi Registry
* NiFi CA Service`
|* Kafka
* Schema Registry
* Streams Messaging Manager
|* ZooKeeper
* HDFS
* YARN
* Spark
* Hive
* Impala
* Kudu
* Hue
* Oozie
|* CDSW
|====

This cluster is meant to be used for demos, experimenting, training, and workshops.

== Namespaces

The automated setup scripts support the concept of *namespaces*. All the resources of a single deployment are associated with a single namespace. By creating multiple namespaces, one can manage multiple independent deployments without them interfering with each other.

A namespace is defined by simply creating a configuration file called `.env.<namespace>`, where `<namespace>` is a string that identifies the namespace uniquely. The namespace identifier cannot contain spaces or special characters.

[[stopping-instances]]
== Stopping and starting instances

IMPORTANT: Please read this section before stopping instances to ensure you can start them again successfully.

After the workshop environment is deployed it may be desirable to stop instances during periods when they are not being used to reduce costs. This feature is *not enabled by default* but it can be turned on for certain deployments.

If you don't enable it correctly, as explained in this section, the clusters will *not* work correctly after a stop/start. So please read on!

==== Requirements

You *must* ensure that the following point are configured correctly before launching your environment:

* Look for the `TF_VAR_use_elastic_ip` in your `.env*` file and set it to `true`. If it doesn't existing in your `.env*` file, ensure the line below is added to it:
+
[source]
----
export TF_VAR_use_elastic_ip=true
----
* Once the property above it set, you must ensure that your AWS environment can allocate as many Elastic IPs as the number of VMs you're launching (# of cluster + 1 for the web server). The AWS default limit for Elastic IPs is very low (5 only), so make sure you check this and request an increase if it's too low for your needs.
** You can check the limit and request the increase in the AWS Console, under *EC2 > Limits > Search for "EC2-VPC Elastic IPs"*.
** If the limit is too low, select the *EC2-VPC Elastic IPs* line, click on *Request limit increase* and follow the prompts.
** The increase can be effective in a few minutes but sometimes it can take longer.

==== Before stopping your instances

If you followed the instructions above all your instances can be stopped after the deployment is complete. To ensure that the workshop services will continue to work correctly after instances are resumed, we recommend that you only stop the instances after the CDSW model deployment is completed.

Before you stop the instances, use the `check-service.sh` command to check the service status and confirm that all services are `Ok` and that the model has status `deployed`, as shown in the example below:

[source]
----
$ ./setup/terraform/check-services.sh default
instance                ip address  WEB   CM    CEM   NIFI  NREG  SREG  SMM   HUE   CDSW  Model Status
aws_instance.web[null]  1.2.3.4     Ok
aws_instance.cluster[0] 1.2.3.5           Ok    Ok    Ok    Ok    Ok    Ok    Ok    Ok    deployed
aws_instance.cluster[1] 1.2.3.6           Ok    Ok    Ok    Ok    Ok    Ok    Ok    Ok    deployed
----

The output `list-detail.sh` command contains a `Stoppable` column, which indicates if the instance can be safely stoppable or not.

[source]
----
$ ./setup/terraform/list-details.sh default
...
WEB SERVER VM:
==============
Web Server Name           Public DNS Name              Public IP  Private IP  Stoppable
araujo-default-web        ec2-1-2-3-4.compute.aws.com  1.2.3.4    10.0.0.1    Yes

CLUSTER VMS:
============
Cluster Name              Public DNS Name              Public IP  Private IP  Stoppable
araujo-default-cluster-0  ec2-1-2-3-5.compute.aws.com  1.2.3.5    10.0.0.2    Yes
araujo-default-cluster-1  ec2-1-2-3-6.compute.aws.com  1.2.3.6    10.0.0.3    Yes
----

==== Starting instances

After you start the instances it takes some time for the services, especially CDSW, to be ready to be used.

Monitor the services status with the `check-service.sh` command described above. If any of the services doesn't come up properly after several minutes, restart it manually through Cloudera Manager.

==== Known issues

The following services can show an unhealthy state after the stop/start. The are not normally used in the workshops, so this doesn't pose any major problem. If needed, they can be manually restarted from Cloudera Manager:

* HBase
* Livy
* Spark
* YARN

== User registration

For security reasons, the workshop environment is created by default without access from the public Internet. The environment includes a Web Server that allows users to register using a registration code. Once they register in the portal the admin is able to open the necessary firewall ports so that they can access their individual clusters.

The admin can control the registration process, opening and closing registration as needed, as explained below.

After launching your environment successfully (see the Setup section below), use the following steps to manage the registration process:

. Before the users can connect to the Web Portal to register it is necessary to allow limited access to the portal from the public Internet. To do this, run the following command:
+
[source,shell]
----
cd edge2ai-workshop/setup/terraform/
./open-registration.sh <namespace>
----
+
This command will allow public access to the Web Portal so that users can connect and start to register. The command will then enter a loop, monitoring the users' registration and will open the needed firewall ports for each individual user's IP address. Leave the command running while users are registering.

. Provide the users with the Web Server address and the registration code displayed on the screen by the command above so that they can start to register.

. Once you confirm that all users have registered, press `ENTER` on the window running the command above. The command will exit the monitoring loop, close the public access to the Web Server and ensure that all the necessary rules in place so that all users can continue to use the environment even without the public access rule.

=== Additional commands

The `open-registration.sh` command above is typically all you need to manage the registration process. The commands below, however, may come handy in certain situations:

* `sync-ip-addresses.sh` - this is the command that runs during the `open-registration.sh` loop to synchronize the user IPs registered in the portal with the environment security groups. If the IPs in the portal get out of sync with the IP rules in the environment's security groups, you can run the following command to sync them:
+
[source,shell]
----
./sync-ip-addresses.sh <namespace>
----

* `close-registration.sh` - if the `open-registration.sh` command didn't finish gracefully for any reason (e.g. window was closed or an error was thrown), the public access to the Web Portal will remain open. To close the public access (and the registration), run:
+
[source,shell]
----
./close-registration.sh <namespace>
----

* `update-registration-code.sh` - used to change the portal registration code if needed:
+
[source,shell]
----
./update-registration-code.sh <namespace>
----

* `manage-ip.sh` - used to manually add or remove IPs to/from the environment security groups:
+
[source,shell]
----
./manage-ip.sh <namespace> add <ip_address>
./manage-ip.sh <namespace> remove <ip_address>
----

== Setup

The setup of the workshop environment is fully automated. Before launching the workshop environment you need to ensure your laptop has the necessary pre-requisites to execute the setup script.

=== Pre-requisites

. Ensure a recent version of link:https://hub.docker.com/editions/community/docker-ce-desktop-mac[Docker] is installed *and* _running_ on your laptop

=== Launching the workshop environment

. Clone this repository
+
[source,shell]
----
# Install Git, skip if you already have it
sudo yum install -y git

# Clone the repo
git clone https://github.com/cloudera-labs/edge2ai-workshop.git
----

. Create a copy of the stack template and name it `stack.sh`, in the `resources` directory:
+
[source,shell]
----
cp edge2ai-workshop/setup/terraform/resources/stack.cdp716p.sh edge2ai-workshop/setup/terraform/resources/stack.sh
----
+
The `stack.sh` file is the default stack definition file used by all the namespaces that lack _namespace-specific_ stacks. You can create namespace-specific stack by naming the file `stack.<namespace>.sh` instead.

. Create a copy of the `.env.template` file and name it `.env.<namespace>`.
+
This file contains the details and credentials for your environment. Ensure the permissions on this file are set to `400`.
+
[source,shell]
----
cp edge2ai-workshop/setup/terraform/.env.template edge2ai-workshop/setup/terraform/.env.<namespace>
chmod 400 edge2ai-workshop/setup/terraform/.env.<namespace>
----
+
where `<namespace>` is an arbitrary name for your environment namespace.

. Edit the `.env.<namespace>` file to match your environment.
+
Ensure you set these variables:
+
--
* `TF_VAR_cluster_count`: number of one-node clusters to be created
* `TF_VAR_launch_web_server`: whether to launch the web server (`true`) or not (`false`).
* `TF_VAR_use_elastic_ip`: enable elastic IPs for the environment VMs. This is required if you need to stop/start VMs.

'''

* `TF_VAR_owner`: your user id. This will be used to tag your cloud resources.
* `TF_VAR_enddate`: value for the `enddate` tag to be added to your cloud resources.
* `TF_VAR_project`: value for the `project` tag to be added to your cloud resources.

'''

* `TF_VAR_aws_region`: AWS region to use
* `TF_VAR_aws_profile`: Your AWS profile name
* `TF_VAR_aws_access_key_id`: (DEPRECATED) Leave blank if profile is specified. Your AWS Access Key Id
* `TF_VAR_aws_secret_access_key`: (DEPRECATED) Leave blank if profile is specified. Your AWS Secret Access Key

'''

* `TF_VAR_web_server_admin_email`: email used by the Web Server admin. This will only be used to identify the admin upon logging in to the Web Server.
* `TF_VAR_web_server_admin_password`: Web Server admin password.
* `TF_VAR_registration_code`: registration code for the workshop environment. This is the code to be used as a password for users to successfully register in the workshop Web Portal. If not set or empty, a random registration code will be generated when the environment is launched.

--

. Launch your environment
+
[source,shell]
----
cd edge2ai-workshop/setup/terraform/
./launch.sh <namespace>
----
+
where `<namespace>` is the name of one of your namespaces.

+
At the end of the script execution it will list the following information for all the clusters. This information should be provided to the workshop attendees:

* Public DNS Name
* Public IP
* Private DNS Name

A private key file will also be created on the local directory for authenticating the connections to the clusters.

A few helper scripts are provided to help managing the clusters. Cluster numbers start from 0 (zero).

* `./list-details.sh [namespace]` - if run without arguments it will display a summary of all the existing environments. If a namespace is specified, it will display the details for all the clusters on that environment (public DNS, public IP and private DNS).
* `./check-services.sh <namespace>` - perform a health check of all the cluster to verify if all the services are up and running.
* `./connect-to-cluster.sh <namespace> <cluster_number>` - connect to the specified cluster using SSH.
* `./browse-cluster.sh <namespace> <cluster_number>` - (MacOS only) Opens a Chrome browser with all the tabs required for the workshop. All the URLs use the cluster's public DNS name.
* `./browse-cluster-socks.sh <namespace> <cluster_number>` - (MacOS only) Same as above, but using URLs with the private DNS name, instead, and setting the browser to use a SOCKS proxy, which is spawn by the script.
* `./run-on-cluster.sh <namespace> <cluster_number> '<command>'` - run a command on the specified cluster.
* `./run-on-all-clusters.sh <namespace> '<command>'` - run a command on all clusters.
* `./upload-instance-details.sh <namespace> [web_ip_adress] [admin_email] [admin_password] [admin_full_name]` - upload all the instances' details to the web server. If no parameters are specified it will use the default web server for the current deployment, otherwise will upload to the specified webserver. Note that this script is automatically executed upon launch for the current web server.

== Use the environment

* Once the script returns, you can open Cloudera Manager at http://<public_dns>:7180. The default credentials are `admin/supersecret1`.

* Wait for about 10-20 mins for CDSW to be ready. You can monitor the status of CDSW by issuing the `cdsw status` command.

* You can use `kubectl get pods -n kube-system` to check if all the pods that the role `Master` is suppose to start have really started.

* You can also check the CDSW deployment status on `CM > CDSW service > Instances > Master role > Processes > stdout`.

== Terminating the workshop environment

. Once the workshop is completed, terminate all the resource in the namespace with the following command:
+
[source,shell]
----
# cd edge2ai-workshop/setup/terraform/
./terminate.sh <namespace>
----
+
where `<namespace>` is the name of one of your namespaces.

== Deploying on an existing VM

The `launch.sh` script used above to create the environment, currently only with AWS and will create all the required AWS resources needed for the workshop.
If you already have virtual machine created somewhere (AWS, GCP or private cloud), you have the option to manually execute the setup script locally on that VM to set up the single-node cluster on it.

NOTE: The steps below assume that you already have a *Centos 7* VM created on the infrastructure of your preference.

To deploy the single-node cluster on that VM following these steps:

. Attach an additional *200GB* volume to this VM but *do not* mount it (this will be used as CDSW's docker device)
. Log in as `root`
. Execute these commands:
+
[source,shell]
----
yum install -y git
git clone https://github.com/cloudera-labs/edge2ai-workshop/
cp -r edge2ai-workshop/setup/terraform/resources /tmp
cd /tmp/resources
----

. Edit the `stack.cdp717p.sh` file that exists in the `resources` folder and update the following variables with your CDP credentials:
+
[source,properties]
----
REMOTE_REPO_USR=<YOUR_USERNAME_HERE>
REMOTE_REPO_PWD=<YOUR_PASSWORD_HERE>
----

. Run the setup script to deployment the cluster:
+
[source,shell]
----
sudo bash -x ./setup.sh <CLOUD_PROVIDER> <SSH_USER> <SSH_PWD> cdp717p <DOCKER_DEVICE>
----

where:

* `CLOUD_PROVIDER` can be one of `aws`, `gcp`, `aliyun`, `generic` or `local`. For private cloud VMs, choose `other`.
* `SSH_USER` is the user you'll use to connect. This user must have passwordless sudo to root
* `SSH_PWD` is the password that the setup script will set for this user. You don't need to set this up for this user.
* `"cdp717p"`, this is the name of the stack. Do not change this, unless you know what you're doing.
* `DOCKER_DEVICE` path of the device created in step 1. Something like `/dev/<devname>`

== Running locally without Docker

If you need or want to run the setup scripts without installing Docker, you should install the following prerequisites in your laptop.

. Install Terraform
+
This setup uses link:https://www.terraform.io/[Terraform] to spin up the VMs and execute the required setup scripts.

.. Check if Terraform is installed and version is 0.12.3 or later
+
[source,shell]
----
terraform version
----
.. If Terraform is not installed or the version is lower, install a later version:
+
[source,shell]
----
# The URL below is for Linux. For Terraform on Mac see www.terraform.io/downloads.html
curl -O https://releases.hashicorp.com/terraform/0.12.6/terraform_0.12.6_linux_amd64.zip
mkdir ./bin
unzip -d ./bin/ terraform_0.12.6_linux_amd64.zip
export PATH=$PWD/bin:$PATH
----

. Install `jq`
.. If you are using a Mac, you can install `jq` using Homebrew:
+
[source,shell]
----
brew update
brew install jq
----

.. Otherwise, see download and install instructions link:https://stedolan.github.io/jq/download/[here]. Make sure `jq` is in your PATH after it's installed.

. Install required Python modules
+
[source,shell]
----
pip install jinja2 pyyaml awscli
----

== Troubleshooting and known issues

=== Clock Offset

The NTPD service which is required by Kudu and the Host is not installed. For the moment, just put
`--use-hybrid-clock=false`  in Kudu's Configuration property `Kudu Service Advanced Configuration Snippet (Safety Valve) for gflagfile` and suppressed all other warnings.

=== Docker device

To find out what the docker device mount point is, use `lsblk`. See below examples:

See examples below:

==== AWS, using a M5.2xlarge or M5.4xlarge VM:

[source,shell]
----
$ lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme0n1     259:1    0  100G  0 disk
+-nvme0n1p1 259:2    0  100G  0 part /
nvme1n1     259:0    0 1000G  0 disk

$ ./setup.sh aws cluster_template.json /dev/nvme1n1
----

==== Azure Standard D8s v3 or Standard D16s v3

[source,shell]
----
$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
fd0      2:0    1    4K  0 disk
sda      8:0    0   30G  0 disk
+-sda1   8:1    0  500M  0 part /boot
+-sda2   8:2    0 29.5G  0 part /
sdb      8:16   0   56G  0 disk
+-sdb1   8:17   0   56G  0 part /mnt/resource
sdc      8:32   0 1000G  0 disk
sr0     11:0    1  628K  0 rom

$ ./setup.sh azure cluster_template.json /dev/sdc
----

==== GCP n1-standard-8 or n1-standard-16

[source,shell]
----
$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0  100G  0 disk 
└─sda1   8:1    0  100G  0 part /
sdb      8:16   0 1000G  0 disk 

$ ./setup.sh gcp cluster_template.json /dev/sdb
----
