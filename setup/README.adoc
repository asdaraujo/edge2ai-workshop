= Single Node EDGE2AI CDH Cluster

This script automatically sets up a CDH cluster on the public cloud on a single VM with the following 16 services: 

[%autowidth,cols="1a,1a,1a,1a,1a",options="header"]
|====
^|CEM ^|CFM ^|CSP ^|CDH ^|CDSW
|* MiNiFi
* EFM
|* NiFi
* NiFi Registry
* NiFi CA Service`
|* Kafka
* Schema Registry
* Streams Messaging Manager
|* ZooKeeper
* HDFS
* YARN
* Spark
* Hive
* Impala
* Kudu
* Hue
* Oozie
|* CDSW
|====

TODO: Update Readme for Packer

As this cluster is meant to be used for demos, experimenting, training, and workshops, it doesn't setup Kerberos and TLS.

== Instructions

Below are instructions for creating the cluster. You can choose between the automated setup, using Terraform, or a manual setup.

* Automated setup - uses Terraform to instantiate VMs in a cloud provider and execute the setup scripts to prepare the environment
* Manual setup - you instantiate the VM with the provided specs and run the setup scripts manually.

== Namespaces

The automated setup scripts support the concept of *namespaces*. All the resources of a single deployment are associated with a single namespace. By creating multiple namespaces, one can manage multiple independent deployments without them interfering with each other.

A namespace is defined by simply creating a configuration file called `.env.<namespace>`, where `<namespace>` is a string that identifies the namespace uniquely. The namespace identifier cannot contain spaces or special characters.

== Automated setup

This setup uses link:https://www.terraform.io/[Terraform] to spin up the VMs and execute the required setup scripts.

. Install Terraform
.. Check if Terraform is installed and version is 0.12.3 or later
+
[source,shell]
----
terraform version
----
.. If Terraform is not installed or the version is lower, install a later version:
+
[source,shell]
----
# The URL below is for Linux. For Terraform on Mac see www.terraform.io/downloads.html
curl -O https://releases.hashicorp.com/terraform/0.12.6/terraform_0.12.6_linux_amd64.zip
mkdir ./bin
unzip -d ./bin/ terraform_0.12.6_linux_amd64.zip
export PATH=$PWD/bin:$PATH
----

. Install `jq`
.. If you are using a Mac, you can install `jq` using Homebrew:
+
[source,shell]
----
brew update
brew install jq
----

.. Otherwise, see download and install instructions link:https://stedolan.github.io/jq/download/[here]. Make sure `jq` is in your PATH after it's installed.

. Install required Python modules
+
[source,shell]
----
pip install jinja2 pyyaml
----

. Clone this repository
+
[source,shell]
----
# Install YUM, skip if you already have it
sudo yum install -y git

# Clone the repo
git clone https://github.com/asdaraujo/edge2ai-workshop.git
----

. Review software versions and edit them as needed. All the versions and locations of the software used for the setup are defined in the `edge2ai-workshop/setup/scripts/stack.sh` file. If this file does not exist, create it with the command below:
+
[source,shell]
----
cp edge2ai-workshop/setup/scripts/stack.template.sh edge2ai-workshop/setup/scripts/stack.sh
----
+
The `stack.sh` file is the default stack definition file used by all the namespaces. You can create namespace-specific definitions by creating the following file for a given namespace:
+
[source,shell]
----
cp edge2ai-workshop/setup/scripts/stack.template.sh edge2ai-workshop/setup/scripts/stack.<namespace>.sh
----
+
IMPORTANT: Most of the software locations are already pre-defined in the template. For a few of the required software, though, there's currently no public repository available. Make sure you follow the instructions below to install these components.
+
For Schema Registry (SR) and Streams Messaging Manager (SMM) installation you have two options:

* If If you have an URL link to a location where the CSP parcel and SR/SMM CSDs can be downloaded from, configure the corresponding properties in the `stack.sh` (or `stack.<namespace>.sh`) file:
+
[source,shell]
----
SCHEMAREGISTRY_VERSION=
STREAMS_MESSAGING_MANAGER_VERSION=
CSP_PARCEL_REPO=
SCHEMAREGISTRY_CSD_URL=
STREAMS_MESSAGING_MANAGER_CSD_URL=
----
+
NOTE: The version of the components, as indicated in the `SCHEMAREGISTRY_VERSION` and `STREAMS_MESSAGING_MANAGER_VERSION` variables, have the form: `<csd_version>.<parcel_version>`. For example, for the CSD binary `SCHEMAREGISTRY-0.8.0.jar` and parcel version `2.0.0.0-112` the Schema Registry version is `0.8.0.2.0.0.0-112`.

* Alternatively, leave the properties above unset and download the CSP parcel into `edge2ai-workshop/setup/parcels/` and both CSDs into `edge2ai-workshop/setup/csds/`. Note that the setup process will upload these files to every VM it creates, so if you're launching a large nuber of VMs, expect a good amount of upload volume.

. Review the cloud and workshop environment definitions and edit them as needed. This information is defined in the
+
[source,shell]
----
cp edge2ai-workshop/setup/terraform/.env.template edge2ai-workshop/setup/terraform/.env.<namespace>
chmod 400 edge2ai-workshop/setup/terraform/.env.<namespace>
----
+
where `<namespace>` is an arbitrary name for your environment namespace.
+
The variables in this file are explained below:
+
--
* `TF_VAR_cluster_count`: number of one-node cluster to be created

'''

* `TF_VAR_owner`: your user id. This will be used to tag your cloud resources.
* `TF_VAR_web_server_admin_email`: email used by the Web Server admin. This will only be used to identify the admin upon logging in to the Web Server.
* `TF_VAR_web_server_admin_password`: Web Server admin password.

'''

* `TF_VAR_aws_region`: AWS region to use
* `TF_VAR_aws_access_key_id`: Your AWS Access Key Id
* `TF_VAR_aws_secret_access_key`: Your AWS Secret Access Key

'''

* `TF_VAR_deploy_cdsw_model`: Whether or not to deploy the CDSW model. If set to `false` CDSW will be installed but the workshop model will *not* be deployment. Default is `true`, which causes the model to be deployed.

'''

* `TF_VAR_cluster_ami`: AMI ID to use for the one-node cluster. Ensure you pick a Centos 7 image.
+
NOTE: This is a vanilla Centos 7 AMI. No other prerequistes are necessary. All the required software will be installed by the setup process.
* `TF_VAR_ssh_username`: The username used to log in to the VM. Typically: `centos`
* `TF_VAR_cluster_instance_type`: Instance type to use for the one-node cluster. Recommended: `m5.4xlarge` or later/larger.

'''

* `TF_VAR_project`: Project name. This is used for instance tagging.
* `TF_VAR_enddate`: End date in MMDDYYYY format. This is used for instance tagging. Some Cloudera environment use this to automatically kill "expired" instances.
--

. Launch your environment
+
[source,shell]
----
cd edge2ai-workshop/setup/terraform/
terraform init # this only has to be executed once after cloning the repo
./launch.sh <namespace>
----
+
where `<namespace>` is the name of one of your namespaces.

+
At the end of the script execution it will list the following information for all the clusters. This information should be provided to the workshop attendees:

* Public DNS Name
* Public IP
* Private DNS Name

A private key file will also be created on the local directory for authenticating the connections to the clusters.

. Once the workshop is completed, terminate all the environments with the following command:
+
[source,shell]
----
# cd edge2ai-workshop/setup/terraform/
./terminate.sh <namespace>
----
+
where `<namespace>` is the name of one of your namespaces.

. A few helper scripts are provided to help connecting to the clusters:

* `./list-details.sh <namespace>` - display the details of all the clusters (public DNS, public IP and private DNS).
* `./check-services.sh <namespace>` - perform a health check of all the cluster to verify if all the services are up and running.
* `./connect-to-cluster.sh <namespace> <cluster_number>` - connect to the specified cluster using SSH.
* `./browse-cluster.sh <namespace> <cluster_number>` - (MacOS only) Opens a Chrome browser with all the tabs required for the workshop. All the URLs use the cluster's public DNS name.
* `./browse-cluster-socks.sh <namespace> <cluster_number>` - (MacOS only) Same as above, but using URLs with the private DNS name, instead, and setting the browser to use a SOCKS proxy, which is spawn by the script.
* `./run-on-cluster.sh <namespace> <cluster_number> '<command>'` - run a command on the specified cluster.
* `./run-on-all-clusters.sh <namespace> '<command>'` - run a command on all clusters.
* `./upload-instance-details.sh <namespace> [web_ip_adress] [admin_email] [admin_password] [admin_full_name]` - upload all the instances' details to the web server. If no parameters are specified it will use the default web server for the current deployment, otherwise will upload to the specified webserver. Note that this script is automatically executed upon launch for the current web server.

Clusters numbers start from 0 (zero).

== Namespaces

The automated setup scripts support the concept of *namespaces*. All the resources of a single deployment are associated with a single namespace. By creating multiple namespaces, one can manage multiple independent deployment without them interfering with each other.

A namespace is defined by simply creating a configuration file called `.env.<namespace>`, where `<namespace>` is a string that identifies the namespace uniquely. The namespace identifier cannot contain spaces or special characters.

== Manual setup

. Provision one VM for the cluster setup

* Create a Centos 7 VM with at least 16 vCPUs/ 64 GB RAM. Choose the plain vanilla Centos image, not a cloudera-centos image.
+
NOTE: This is a vanilla Centos 7 AMI. No other prerequistes are necessary. All the required software will be installed by the setup process.
* OS disk size: at least *100 GB*.
* Docker device disk: at least *200 GB* SSD disk.
+
NOTE: You need a fast disk for the *Docker volume*. Aim for a disk with 3000 IOPS. This might mean choosing a 1TB disk.
+
IMPORTANT: Ensure that you allocate disks with the sizes specified above, or larger. Otherwise, you may run out of space during the workshop.

. Configure VM and networking

* If you created the VM on Azure and need to resize the OS disk, here are the [instructions](scripts/how-to-resize-os-disk.md).
* Add 2 inbound rules to the Security Group:
** to allow your IP only, for all ports.
** to allow the VM's own IP, for all ports.

. SSH into the VM, clone this repository and `cd` to the scripts directory:
+
[source,shell]
----
# Install YUM, skip if you already have it
sudo yum install -y git

# Clone the repo
git clone https://github.com/asdaraujo/edge2ai-workshop.git

# Go to the scripts directory
cd edge2ai-workshop/setup/scripts
----

. Review software versions and edit them as needed. All the versions and locations of the software used for the setup are defined in the `edge2ai-workshop/setup/scripts/stack.sh` file. If this file does not exist, create it with the command below:
+
[source,shell]
----
cp stack.template.sh stack.sh
----
+
IMPORTANT: Most of the software locations are already pre-defined in the template. For a few of the required software, though, there's currently no public repository available. Make sure you follow the instructions below to install these components.
+
For Schema Registry (SR) and Streams Messaging Manager (SMM) installation you have two options:

* If If you have an URL link to a location where the CSP parcel and SR/SMM CSDs can be downloaded from, configure the corresponding properties in the `stack.sh` file:
+
[source,shell]
----
SCHEMAREGISTRY_VERSION=
STREAMS_MESSAGING_MANAGER_VERSION=
CSP_PARCEL_REPO=
SCHEMAREGISTRY_CSD_URL=
STREAMS_MESSAGING_MANAGER_CSD_URL=
----
+
NOTE: The version of the components, as indicated in the `SCHEMAREGISTRY_VERSION` and `STREAMS_MESSAGING_MANAGER_VERSION` variables, have the form: `<csd_version>.<parcel_version>`. For example, for the CSD binary `SCHEMAREGISTRY-0.8.0.jar` and parcel version `2.0.0.0-112` the Schema Registry version is `0.8.0.2.0.0.0-112`.

* Alternatively, leave the properties above unset and download the CSP parcel into `edge2ai-workshop/setup/parcels/` and both CSDs into `edge2ai-workshop/setup/csds/`. Note that the setup process will upload these files to every VM it creates, so if you're launching a large nuber of VMs, expect a good amount of upload volume.

. Run the `setup.sh` script. It takes 3 arguments:
+
IMPORTANT: The script current implementation only supports AWS deployments. Azure and GCP will be added in the future.

** The cloud provider name: `aws`,`azure`,`gcp`.
** The template file.
** (OPTIONAL) the Docker Device disk mount point.

+
--
_Example_:

[source,shell]
----
chmod +x setup.sh
./setup.sh aws cluster_template.json /dev/sdc
----
--

. Wait until the script finishes, check for any error.

== Use

* Once the script returns, you can open Cloudera Manager at http://<public_dns>:7180. The default credentials are `admin/admin`.

* Wait for about 10-20 mins for CDSW to be ready. You can monitor the status of CDSW by issuing the `cdsw status` command.

* You can use `kubectl get pods -n kube-system` to check if all the pods that the role `Master` is suppose to start have really started.

* You can also check the CDSW deployment status on `CM > CDSW service > Instances > Master role > Processes > stdout`.

== Other setup scripts

=== SMM Truck Demo
The scripts necessary to run the SMM Truck Demo are deployed to the cluster instances upon launch but are *not* executed.

To complete the setup for the SMM Truck Demo, follow the steps below:

==== Run for all clusters.

* Go to the terminal window where you launched the workshop (under the `setup/terraform` directory)
* Run the SMM Truck Demo setup (only needed once):
+
[source,shell]
----
./run-on-all-clusters.sh <namespace> "sudo /opt/dataloader/smm-generator.sh setup"
----
+
--
The setup will:

* Deploy all the necessary scripts and files
* Load the NiFi flows used in the demo
* Start all the NiFi Controller Services
--

After the setup is complete, you can start/stop the consumer and producers as many times as needed:

* To start all the consumers and producers:
+
[source,shell]
----
./run-on-all-clusters.sh <namespace> "sudo /opt/dataloader/smm-generator.sh start"
----

* To stop all the consumers and producers:
+
[source,shell]
----
./run-on-all-clusters.sh <namespace> "sudo /opt/dataloader/smm-generator.sh stop"
----

* You can check the producer and consumer status using:
+
[source,shell]
----
./run-on-all-clusters.sh <namespace> "sudo /opt/dataloader/smm-generator.sh status"
----
+
--
After a successful start there should be 43 clients running: 13 consumers and 30 producers. There are 2 different types of consumers and 2 of producers. The status command shows the number of running consumers and producers by type, as shown below:

[source,python]
----
      3 LoggerAvroEventConsumer
     10 LoggerStringEventConsumer
      9 SMMSimulationRunnerSingleDriverApp
     21 SMMSimulationRunnerTruckFleetApp
----

--

==== Run for a single node

You can also run the commands above for a single instance.

1. SSH to the cluster instance
2. Run the commands as per below:

* *Setup*: `sudo /opt/dataloader/smm-generator.sh setup`
* *Start*: `sudo /opt/dataloader/smm-generator.sh start`
* *Stop*: `sudo /opt/dataloader/smm-generator.sh stop`
* *Status*: `sudo /opt/dataloader/smm-generator.sh status`

== Troubleshooting and known issues

=== Clock Offset

The NTPD service which is required by Kudu and the Host is not installed. For the moment, just put
`--use-hybrid-clock=false`  in Kudu's Configuration property `Kudu Service Advanced Configuration Snippet (Safety Valve) for gflagfile` and suppressed all other warnings.

=== Docker device

To find out what the docker device mount point is, use `lsblk`. See below examples:

See examples below:

==== AWS, using a M5.2xlarge or M5.4xlarge VM:

[source,shell]
----
$ lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme0n1     259:1    0  100G  0 disk
+-nvme0n1p1 259:2    0  100G  0 part /
nvme1n1     259:0    0 1000G  0 disk

$ ./setup.sh aws cluster_template.json /dev/nvme1n1
----

==== Azure Standard D8s v3 or Standard D16s v3

[source,shell]
----
$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
fd0      2:0    1    4K  0 disk
sda      8:0    0   30G  0 disk
+-sda1   8:1    0  500M  0 part /boot
+-sda2   8:2    0 29.5G  0 part /
sdb      8:16   0   56G  0 disk
+-sdb1   8:17   0   56G  0 part /mnt/resource
sdc      8:32   0 1000G  0 disk
sr0     11:0    1  628K  0 rom

$ ./setup.sh azure cluster_template.json /dev/sdc
----

==== GCP n1-standard-8 or n1-standard-16

[source,shell]
----
$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0  100G  0 disk 
└─sda1   8:1    0  100G  0 part /
sdb      8:16   0 1000G  0 disk 

$ ./setup.sh gcp cluster_template.json /dev/sdb
----
